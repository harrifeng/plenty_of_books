#+OPTIONS:^:{}
* Chapter 1 Introduction to the Linux Krenel
* Chapter 2 Getting Started with the Kernel
* Chapter 3 Process Management
** The Process
   + 进程简言之就是正在执行的程序和它所占有的资源的总和, 它所占有的资源,
     简单来说有如下:
     1) 打开的文件
     2) 挂起的信号
     3) 内核内部的数据
     4) 处理器的状态
     5) 内存地址(包括内存的映射关系)
     6) 一个或多个线程
     7) 存放全局变量的数据段
   + 线程一般是进程的一部分,但是在Linux系统中不区分进程和线程,对于Linux来说,
     线程只是碰巧恭喜资源的一些进程
   + 现代操作系统提供了两个虚拟:
     - 让所有的process认为自己独享CPU
     - 让所欲的process认为自己独享内存
   + 所有的process都是从fork()开始的, fork()这个函数很特别,从内核中返回两次,一次
     在parent,一次在child.当前的Linux是通过clone()来实现fork()的
   + 在chlid中,fork返回后,一般都是执行新的程序, exec()系列函数负责这个部分
   + 当process结束的适合,会调用exit()函数. 所有调用了exit()的process都要进入
     一个叫做zombie的状态来等待他们的parent调用wait()系列函数来释放他们的资源
** Process Descriptor and the Task Structure
   + 每一个process的信息都写在process descriptor里面(descriptor是一个unix理念,
     基本所有的对象都有一个descriptor), 在Linux里面process descriptor的类型不再
     是一个简单的类型,而是一个struct: struct task_struct
   + 在32位机器上面,这个task_struct有1.7K大小,因为他保存了一个process所占有的所有
     的资源(1打开的文件,2挂起的信号...)
** Allocating the Process Descriptor
   + 为了达到对象重用和缓存着色(Cache Coloring)的目的, Linux使用slab allocator
     来动态的分配task_struct.
   + 每个进程都有一个自己的进程内核栈,原来task_struct是静态分配的,那么就存放在每个
     内核栈的最后面(最不可能被用到的地方, 同时也方便找到).因为2.6以后, task_struct
     是动态分配的,地址都是不定的.所以很自然的我们想到在原来内存栈最后面的地方存放一个
     指针来指向task_struct.后来为了增加信息把这个指针扩充成了一个新的结构体
     task_info(这个结构体第一个成员就是指向task_struct的指针)
   + task_info代码如下:
     #+begin_src c
       struct thread_info {
           struct task_struct      *task;
           struct exec_domain      *exec_domain;
           __u32                   flags;
           __u32                   status;
           __u32                   cpu;
           int                     preempt_count;
           mm_segment_             addr_limit;
           struct restart_block    restart_block;
           void                    *sysenter_return;
           int                     uaccess_err;
       }
     #+end_src
   + 整体示意图如下:
     #+begin_example
                     Process Kernel Stack
              +------------------------------+ -- highest memory address
              |       Start of Stack         |
              |              |               |
              |              |               |
              |              |               |
              |              |               |
              |              |               |
              |              |               |
              |              |               |
              |              |               |
              +--------------+---------------+ -- stack pointer
              |              |               |
              |              V               |
              |                              |
              |                              |
              |                              |
              |                              |
              +------------------------------+
              |   struct thread_info {       |
              |     struct task_struct *task;|
              |     //.......                |
              |   }                          |
              |                              |
              +------------------------------+ -- lowest memory address
     #+end_example
** Storing the Process Descriptor
   + 每一个进程都是有一个ID来标示的,类型是pid_t为了和老的Unix系统兼容, pid_t通
     常都是int,这也就意味着系统同时最大共存的进程数目是32768
   + 如果希望能有更多的进程共存,那么可以修改/proc/sys/kernel/pid_max
   + 内核中河task_struct直接沟通的情况非常多,那么直接能够得到当前的task_struct
     就变的很必要.
     - 在power pc等寄存器多的系统中,一般当前的task_struct地址就存在r2这个寄存器中.
     - 在x86这种系统中,我们可以用刚才上面讲解的thread_info在进程内核栈最开始这个有
       利的布局,通过当前内核栈开始位置减去内核栈大小就得到了thread_info的位置,然后
       current_thread_info()->task就得到了task_struct的地址.
** Process State
   + task_struct里面有很多重要的变量,比如这个state,它表达了process可能处于的如下
     几种状态:
     - TASK_RUNNING : 正在运行或者在run-queue上等待运行
     - TASK_INTERRUPTIBLE : 程序正在睡眠(也就是被阻塞了),等待某个condition出现
       继而可以重新进入TASK_RUNNING状态.因为是INTERRUPTIBLE,所以proceess也可能
       接受到某个信号而重新进入TASK_RUNNING
     - TASK_UNINTERRUPTIBLE : 和上面的TASK_INTERRUPTIBLE一样,除了无法接受中断
       而唤醒.(我们使用ps命令看到的在state里面显示为D的,就是这种状态的process, 因
       为连信号都无法中断它,所以你发送SIGKILL也是无法杀掉这个进程的.即便有办法杀掉,
       杀掉这样的进程也不合理,因为它可能正在一个很重要的操作中间,而且可能持有一个信
       号量. 使用场景:
       1) 在一个process必须等待而不能被中断的场景
       2) 要等待的context应该会马上到来
     - __TASK_TRACED : 进程正在被其他进程跟踪
     - __TASK_STOPPED : 进程的执行已经被终止:
       1) 要么是收到SIGSTOP, SIGTSTP, SIGTTIN, SIGTTOU
       2) 要么是在debug的适合收到任何的信号
   + 这些状态的转换图如下:
     #+begin_example
       +-----------------+                                    +-----------------+
       |Existing task    |                                    |Task is          |
       |calls fork() to  |                                    |Terminated       |
       |create new task  |   Scheduler dispatches task to run |                 |
       |                 |   schedule() calls context_switch()|                 |
       +---+-------------+ +-------------------------------+  +-------------^---+
           |fork()         |                               |                |
           |    +----------+------+                +-------V---------+      |
           |    | TASK_RUNNING    |                |  TASK_RUNNING   |      |
           |    | (ready but not  |                |   (running)     |      |
           +---->  running)       |                |                 +------+
                |                 |                |                 |do_exit()
                +-^-----^---------+                +-------+--------++
                  |     |Task is preempted by higher prior |        |
                  |     +----------------------------------+        |
                  |                                                 |Task sleeps
                  |                                                 |on wait que
                  |        +-----------------------------+          |for a given
                  |        |  TASK_INTERRUPTIBLE         |          |event
                  |        |        or                   |          |
                  +--------+  TASK_UNINTERRUPTIBLE       |          |
        Events occurs, tsk |                             <----------+
        is woken up, placed+-----------------------------+
        back to runqueue.
     #+end_example
** Manipulating the Current Process State
   + 在Linux中要考虑多核的情况,所以不能直接task->state = state, 需要一个函数来完成
     #+begin_src c
       set_task_state(task, state);
       set_current_state(state);
     #+end_src
** Process Context
   + 所谓"进程上下文"是指的,当process开始运行的时候一般是在用户空间, 而用户空间的功能
     是有限的,它会用到内核的功能,这个时候可以通过:
     1)系统调用
     2)触发异常
   + 当我们通过上面的方式进入内核态的时候,内核其实是帮用户态的process在工作,我们就说
     这种工作状态为"进程上下文"(内核代表进程执行)
   + 在进程上下文中, current宏是有效的, 因为内核是知道在帮助哪个用户的process在工作,
     后面讲到的中断上下文(interrupt context), current就没有用了,因为不是为某个
     proces工作
** The Process Family Tree
   + 每个进程都有一个父进程, 而每个进程也会有0到多个子进程.在task_struct里面分别通过
     变量parent 和 child来访问
   + 所有的进程都有一个"跟进程"
     那就是init, 在Linux里面叫init_task,PID==1.
** Process Creation
   + 在类Unix系统中, 进程的创建都很独特: 其他操作系统一般是在新的用户空间创建新
     的进程. 而Unix把这个工作分成了两步:
     1) Fork: 创建一个子进程,只有一些参数和父进程不一样(parentID, PID不会继承挂起的信号也不会继承)
     2) exec():把新的可执行程序导入到地址空间开始执行
** Copy-on-Write:
   + 我们说Unix系统进程创建之所以独特,也就是独特在fork上面,因为传统的fork会吧所有
     的父进程的资源复制一遍给子进程.因为子进程并不需要那么多的资源,而且很可能
     子进程要重启新的炉灶(新的可执行程序),所有的资源都要放弃. 这个时候我们就
     引入了"写时拷贝"
   + 所谓"写时拷贝"就是fork的时候,资源是只读的,父子进程共享地址, 一旦这些资源被
     写入了,那么说明一份新的数据会诞生.那么父子就不能在使用相同的地址了. 这个时候
     就要再复制一份新的数据. 因为很多情况下, fork之后马上是exec(), 所以数据的复
     制从头到尾都没有执行过.
   + fork其实只需要给子进程复制一下page table, 和创建一些新的process descriptor
     里面的变量而已,所以速度要快,这符合Unix快速创建进程的哲学
** Forking
   + Linux的fork==>clone(), clone()通过一些flag来判断父子进程共享哪些资源
   + clone()内部==>do_fork()
   + do_fork()==>copy_process(), 真正的工作在这里执行:
     - 调用dup_task_struct(), 创建如下:
     - 检查一下创建新进程后,当前用户的资源有没有超标
     - 因为刚才的数据都是复制的,父子的task_struct是一样的,但是为了区分父子,肯定有很多属
       性要不一样:比如一些计数的属性要清零. 但大部分还是一样的.这里主要是属性,不是
       "写时拷贝"的那些数据
     - 子进程被设置为TASK_UNINTERRUPTIBLE防止它运行
     - copy_process()==>copy_flag() 更新子进程的flags: 清零PF_SUPERPRIV(表明进程是
       否拥有超级用户权限), 清零PF_FORKNOEXEC(表明还没有调用exec())
     - 根据传入clone()的flag的不同, copy_process()会选择对以下的资源才去复制一份给子
       进程,还是共享(一般来说如果是thread的话,那么这些信息是共享的,在Linux系统中就是如
       果几个process共享这些信息,那么他们就是thread):
     - copy_process()会释放资源然后返回给调用者一个指向新子进程的指针.
   + copy_process()<==do_fork()完成任务返回了do_fork, 如果copy_process()调用成功
     那么新的子进程会比父进程先唤醒, 这是基于这样一个事实: 如果父进程先唤醒,那么可能会
     写入address space, 这样一来,就会调用写时拷贝来复制一份资源!如果子进程先调用,那么
     这些内容本是可以避免的.因为子进程可能上来就放弃了这些资源,调用exec()开始新的生活!
** vfork()
   + vfork()是一种在copy-on-write技术诞生之前的应对"两步生成新process"的方案, 现在
     已经不需要了.Linux在2.2之前vfork()甚至只是通过fork()实现的.
   + vfork()主要是添加了很多限制:
     1) 子进程和父进程公用地址空间(页表不创建新的)
     2) 知道子进程退出或者执行exec(), 父进程都是被阻塞的.
     3) 子进程不允许写入到地址空间里面.
** The Linux Implementation of Threads
   + 在Linux中, 线程只是碰巧共享一些资源(比如地址空间)的一群进程
** Creating Threads
   + 线程只是在创建的时候传给clone()不同的flage而已,比如我们要创建一个子线程(新
     创建的子进程和原来的父进程共享资源,他们一块叫做线程):
     #+begin_src c
       /**************************************************************/
       /* CLONE_VM -> Share address space                            */
       /* CLONE_FS -> Share file system information                  */
       /* CLONE_FILES -> Share open files                            */
       /* CLONE_SIGHAND -> Share blocked signals and signal handlers */
       /**************************************************************/
       clone(CLONE_VM | CLONE_FS |CLONE_FILES |CLONE_SIGHAND, 0);
     #+end_src
** Kernel Threads
   + 内核线程也是进程的一种,也有task_struct,也会被调度,也有那么多state, 它特别的
     地方在于
     - 它没有address space(task_struct的成员mm指针为NULL)
     - 它不会被交换到用户态
   + 内核线程只能由其他内核线程fork创建,在bash里面可以通过ps -ef来查看他们.
** Process Termination
   + process也要消亡,其方式也就是主动和被动两种:
     - 主动调用exit(), 或者被其他语言比如C,在main的最后加上一个exit()
     - 收到了一个信号,或者异常,自己无法处理,又不能忽略
   + 所有的进程结束都是通过do_exit()来处理的
     - 首先是标记flags为PF_EXISTING,表示正在退出
     - 释放掉资源,内存,文件,信号量等等
     - 设置exit code
     - 通知父进程,state成为EXIT_ZOMBIE
     - 调用schedule()执行其他程序, 因为当前程序不可调度了,所以这个函数永远都
       不会返回.
   + 到这个阶段,已经进入了EXIT_ZOMBIE状态,也不能run了, resource也都没有了.这个时候
     一个进程所占有的资源就剩下传统三强了:
     1) 内核栈
     2) thread_info(其实也是在内核栈里面)
     3) task_struct 结构体
** Removing the Process Descriptor
   + 在do_exit()完成后,很多信息都已经删除了,但是还是保留了task_struct,就是因为希望在
     进程退出后,依然能够得到关于它的消息, 当父进程得知子进程退出后,会调用wait()
     来释放最后的这些资源
   + wait()函数的标准动作是挂起,以等待其中一个子进程返回, 同时会返回结束进程的PID, 从
     参数中返回的指针还能知道子进程的exit code
   + 当真要释放process descriptor的时候, release_task()会被调用:
     1) release_task()==>__exit_signal()==>__unhash_process()==>detach_pid(),
	把进程从pidhash已经task list中删除
     2) __exit_signal()会释放已经退出的进程的所有资源
     3) 如果这个是thread group的最后一员,而且leader已经zombie了,那么就通知zombie
        leader的父进程
     4) release_task()==>put_task_struct()会把传统三强铁山角(内核栈, thread_info,
        以及task_struct)释放掉
** The Dilemma of the Parentless Task
   + 如果父进程在子进程之前就退出了,那么我们可以:
     1) 从当前的thread groupd里面找一个进程做自己的父进程
     2) 让PID=1的init来做自己的父进程
* Chapter 4 Process Scheduling
** Multistasking
   + 多任务操作系统能在单核计算机上展现出所有进程共同运行的假象, 在多核计算机上,
     让多个进程真正并行的进行运算
   + 多任务操作系统分成两类:
     - 飞抢占式多任务(cooperative multitasking):调度器能决定哪个进程结束,哪个
       进程开始.
     - 抢占式多任务(preemptive multitasking):进程一旦开始运行,就只有它自己主动
       让出cpu, 其他进程无法抢占
   + 绝大部分操作系统都是抢占式的,Linux也是
** Linux's Process Scheduler
   + 早期linux的调度设计方法简单
   + 2.5开始设计出O(1)调度算法,能够很好的应多多核
   + 后来发现O(1)调度算法对于人机交互的进程很不友好,最终引入了CFS(Completely
     Fair Scheduler)
** I/O-Bound Versus Processor-Bound Processes
   + 所谓I/O-Bound的进程就是真正运行的不多,总是在等IO的进程.这种进程调度的时候
     就要多给他机会,但是每次时间都不要太长
   + 所谓Perocessor-Bound的进程,就是每次调度都是在不停的运行, 这种进程调度的时
     候要每次多给时间, 但不要多给机会.
** Process Priority
   + 最常见的调度算法就是"基于优先级的调度", 就是把所有的进程都根据其价值需求,进行
     分机.优先级高的运行的早,运行时间多
   + 这种最朴素的理念在Linux中也有体现, Linux有两中优先级值:
     - nice value : 默认值是0, 区间是[-20, 19], 值越大就是对其他人越nice, 也就
       优先级越低. 不同Unix系统对于nice的利用不同. Mac OSX 根据nice值来确定运行
       时间. Linux根据nice值来确定处理器使用的比例.
     - 实时优先级 :区间是[0-99], 这个是值越大优先级越高. 跟nice是两个不同的系统.
       任何实时进程优先级都大于普通进程
** Timeslice
   + 我们前面说过传统的操作系统会给每个进程一个timesilce, 而timeslice越长,交互
     程序的体验就越差,所以现在操作系统中的时间片都非常的小--比如10微秒
   + Linux不是分配时间片的,而是分配cpu使用比例,分配的原则:nice 值为主,大家都要兼顾
     也就是说一个进程能获得多少cpu使用比例,要
     1) 看自己的nice值是不是够高
     2) 还要看系统中其他的进程跟自己比起来值大还是小
     3) 系统中进程多不多.僧多粥就少
   + 大部分操作系统下,当某个进程进入可执行状态的时候,看它是不是取代当前的进程运行.主要
     是看它的优先级,以及时间片是否足够
   + Linux没有时间片的概念.进程是否运行是看它使用的CPU的比例,如果新加入可运行的进程
     CPU使用比例比当前的进程低,那么马上就能运行.
** The Scheduling Policy in Action
   + 举个例子.一个文字编辑器和一个视频解码器,前者是IO bound, 后者是CPU bound, 假设
     系统中只有他们俩,而且nice值相同,那么原始情况下没人的cpu 使用比例都是50%. 开始假设
     编辑器先运行, 很快它就会等待IO让出CPU, 所以cpu使用比例也没多少==>1 % now. 视频
     解码马上开始运行. 然后占用的cpu比例很大==> 35%. 这个时候文字编辑器又收到了IO返回
     因为他的cpu使用率远远小于视频解码器,所以它能立刻抢占视频解码器.
** Scheduler Classes
   + Linux的调度器是模块化的.好让不同的调度器去调度不同类型的进程
** Process Scheduling in Unix Systems
   + 传统的Unix调度比较粗犷.每个process给个时间片和优先级,这样做有很多问题:
     1) 因为有级别和时间片,优先级高的进程和优先级低的进程共存的时候没问题.但是如果所有进程
        都是低优先级,那么调度就会非常频繁(比如刚才编辑器解码器那个例子,如果两者时间片很短,
        那就调度台频繁了.Linux肯定没这个问题嘛,一人50%)
     2) 数据分布不均匀. nice值在尾端的进程之间时间片差距太大.
     3) 是节拍器的整数倍,导致进程间的差距不一
     4) 给交互进程开了后门.
** The Linux Scheduling Implementation
   + 下面介绍CFS的四个部分
*** Time Accounting:
    + 虽然CFS不再有时间片的概念,但是还是要记录用掉的时间,一遍能保证大家相对的公平, CFS
      使用shed_entity来记录进程数目:
      #+begin_src c
        struct sched_entity {
            struct load_weight      load;
            struct rb_node          run_node;
            struct list_head        group_node;
            unsigned int            on_rq;
            u64                     exec_start;
            u64                     sum_exec_runtime;
            u64                     vruntim;
            u64                     prev_sum_exec_runtime;
            //....
        }
      #+end_src
    + sched_entity存在在task_struct里面叫做se.
*** The Virtual Runtime
    + 上面se其中一个成员变量是保存进程的虚拟运行时间的.他是实习运行时间加权运行进程数目
      得到的值.
    + virtual runtime的单位是ns, 和timer的节拍就没有关系了. 这个vruntim的设计是为了
      大约估计每个process的运行虚拟时间.因为我们的多核cpu不是理论中那么完美,理论情况下
      同一个权限的process应该一直都有一样的vruntim, 也就不用再去计算,然后相互平衡了.
    + 每次每个process变成runable, unrunnable或者block的时候, 就会调用update_curr()
      来进行修正se里面的vruntime.
*** Process Selection
    + 我们知道, 因为我们不可能实现完全的多任务,所以Linux为了平衡process, 采取了非常
      简单的选择process的策略:谁的virtual time最小下面就先让谁运行.
    + CFS 使用了红黑树来管理所有的runnable的process, 可以迅速的找到有最小vruntime值
      的process.
*** Picking the Next Task
    + Linux使用函数__pick_next_entity()来找到下一个运行的process, 从代码中我们可以看
      到每次并不是真的找到红黑树最左下的进程,这个值其实是被rb_leftmost变量缓存的
      #+begin_src c
        static struct sched_entity *__pick_next_entity(struct cfs_rq *cfs_rq) {
            struct rb_node *left = cfs_rq->rb_leftmost;
            if (!left) {
                return NULL;
            }
            return rb_entry(left, struct sched_entity, run_mode);
        }
      #+end_src
    + 如果leftmost什么值都没有的话,返回NULL, 这个时候CFS会去调度idle task
*** Adding Process to the Tree
    + 没当有新的process被创建,或者process被唤醒的时候, 我们要把新的proces加入到红黑树
      里面去, 这个红黑树的key就是vruntime,
    + 这个把process加入红黑树的函数和其他没有什么区别, 唯一不同的是,我们不在乎vruntime
      冲突,一样的vruntime那就放到一块.
*** Removing Processes from the Tree
    + 当有process介绍, 或者process被block的时候, 我们i就可以从红黑树里面移除这个
      process
*** The Scheduler Entry Point
    + 调度的核心函数是schedule(), 定义在kernel/sched.c, 而schedule()函数主要的
      工作都是由pick_next_task这个函数完成的
      #+begin_src c
        /*
         ,* pick up the hightest-proi task
         ,*/
        static inline struct task_struct *
        pick_next_task(struct rq *rq)
        {
            const struct sched_class *class;
            struct task_struct *p;
            /*
             ,* 下面这段代码是说,我们总共有nr_running个process在跑,如果cfs也有
             ,* 这么多的process在跑,说明所有的进程都是CFS在调度,都是普通进程,没有
             ,* real time的(这也是大多数的情况)
             ,*/
            if (likely(rq->nr_running == rq->cfs.nr_running)) {
                p = fair_sched_class.pick_next_task(rq);
                if (likely(p))
                    return p;
            }

            class = sched_class_highest;
            for (; ;) {
                /*
                 * 每个class里面的pick_next_task和总的这个不是一个函数,比如CFS
                 * 就是用pick_next_entity()来实现pick_next_task()的.
                 */
                p = class->pick_next_task(rq);
                if (p) {
                    return p;
                }
                /*
                 ,* 不可能返回NULL值, 因为总有一个叫idle 的 schedule class
                 ,*/
                class = class->next;
            }
        };
      #+end_src
*** Sleeping and Waking Up
    + 一旦process不想运行,就sleep, 然后就会把自己从调动红黑树中删除,加入自己到等待队列
      (Wait Queue), 然后调用schedule()
    + 一旦被唤醒那就是相反, 设置runnable, 从等待队列删除, 插入红黑树,是否调用schedule那
      就暂时不知道了
*** Wait Queues
    + 在内核中,进入睡眠是通过把自己加入到等待队列里面, 睡眠和唤醒的实现要非常小心, 因为很
      可能会引入竞争.
      #+begin_src c
        /* 'q' is the wait queue we wish to sleep on */
        DEFINE_WAIT(wait); // create one queue node statically

        add_wait_queue(q, &wait);
        while (!condition) { //防止虚假唤醒存在
            prepare_to_wait(&q, &wait, TASK_INTERRUPTIBLE);
            if (signal_pending(current)){
                /* handle singnal */
            }
            //肯定要交出cpu了,因为自己的state已经不是runable了,
            //vruntime再小也不会轮到自己运行了.
            schedule();
        }
        //condition发生了,重新调度到自己,把自己解放出来
        finish_wait(&q, &wait);
      #+end_src
*** Waking Up
    + 我们是通过wake_up()来激活某一些特定的进程的(它们通常加入到同一个queue里面)
    + wake_up()==>
      1) try_to_wake_up(), 设置TASK_RUNNING
      2) enqueue_task(),加入到红黑树
      3) 设置need_resched, 如果有更需要运行的进程.
    + 一般谁引发某个event,它就会来调用wake_up()比如数据从硬盘上来的实惠, VFS会
      调用wake_up()
    + 一个process被wake up,并不代表着它所等待的event就发生了,因为存在着虚假唤醒
      (Spurious wake-ups): 也就是说存在一种可能,我们的process被唤醒了,但是其实它
      所等待的condition还没真的发生, 所以我们要用一个while循环来保护, 在wait退出
      以后继续检查一遍condition,double check一下.
    + 这里的while保护机制,其实是多线程编程的要求. 而不仅仅是内核这么做.比如在下例
      中, 正常情况下, 其他thread肯定会先设置full为true, 然后出发signal, 但是:
      1) 在多线程的情况下,并不总是线性运行
      2) 程序员可能犯错误,把设置为full写到signal后面,我们多做一次check何乐而不为呢.
      #+begin_src c
        /* In any waiting thread */
        while (!buf->full) {
            wait(&buf->cond, &buf->lock);
        }

        /* In any other thread; */
        if (buf->n >= buf->size) {
            buf->full = 1;
            signal(&buf->cond);
        }
      #+end_src
    + 下面就是运行和睡眠状态的转换情况情况总结
      #+begin_example
           __add_wait_queue() adds task to a wait queue, sets the task's
           state to TASK_INTERRUPTIBLE, and calls schedule(). schedule()
           calls deactivate_task() which removes the task from the runqueue
           +---------------------------------------------------+
		   |												   |
		   |												   |
		   |												   |
		   |												  \|/
		+--+-----------+								  +----.------------+
		|			   | and task executes signal handler |					|
		| TASK_RUNNING |<--------------------------------->TASK_INTERRUPTIBLE
		|			   |								  |					|
		+--.-----------+								  +----+------------+
		  /|\												   |
		   +---------------------------------------------------+
            Event the task is waiting for occurs, and try_to_wake_up()
            sets the task to TASK_RUNNING, calls activate_task() to
            add the task to a runqueue, and calls schedule().
            __remove_wait_queue() removes the task from the wait queue.
      #+end_example
*** Preemption and Context Switching
    + schedule()==>context_switch()进行上下文切换:
      - 调用switch_mm(), 切换新老task的virtual memory mapping
      - 调用switch_to(), 切换新老task的处理器state
    + 一个task不能完全靠自觉性来决定自己占用cpu多久.每当need_resched flag被设置
      就必须进行进程切换了,这个flag不可能被正在运行task进行设置, 设置的肯定是能和
      cpu同时运行的其他部件:
      - 频率始终可以通过scheduler_tick()来设置这个flag, 这种情况通常是process
        运行的够长了.
      - 外部的中断处理可以通过try_to_wake_up()来设置个flag, 这种情况是有新的
        优先级更高的task出现了
    + 返回用户空间,或者从终端返回的话,都要去检查need_resched.
    + need_resched这个是每个process的task_struct里面都有一份的.虽然全局又一个
      flag就可以了,但是每个process都有一个的话,用起来更快(current的存在)
*** User Preemption
    + 当从内核空间跳入到用户空间的时候(要么从system call返回,要么是从interrupt返
      回), 会去检测need_resched, 因为这个时候kernel转到进程A是安全的,那么如果需
      要调度,我转到进程B也肯定是安全的.
*** Kernel Preemption
    + 和其他Unix变体不同, Linux的内核也是可抢占的.任何"安全状态"下的kernel进程
      (也就是kernel在为某个进程服务而运行,比如为某个进程运行system call)都是可
      以被抢占的
    + 所谓"安全状态"下的kernel是指:没有lock
    + 从2.6开始支持抢占内核,为此在每个thread_info里面增加了一个变量preempt_count
      来记录当前task拥有的lock.每当此数字为0的时候,内核就是可抢占的.
    + 如果从interrupt返回到用户空间的话,那么check need_resched然后决定是否调度
      就是前面讲的user preemption, 如果interrupt返回到内核空间的话,如果
      preempt_count为0的话,说明没lock,此时也是可以check need_resched调度的,
      如果不需要调度,那么就返回原本该返回的位置.
    + 也可以显式调用内核抢占,比如当在内核中的task被block,或者调用schedule()的时候,
      这种情况下, 抢占是一直发生的.无论锁的情况怎样.
    + 总结一下内核抢占发生的情形就是:
      - 当有中断处理退出并且中断处理返回的是内核空间
      - 当内核变的可以被抢占preempt_count为0
      - 当内核task显式调用schedule(), 自愿放弃cpu
      - 当内核task被block(也就会导致schedule())

      

