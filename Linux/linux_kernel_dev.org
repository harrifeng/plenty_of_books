#+OPTIONS:^:{}
* Chapter 1 Introduction to the Linux Krenel
* Chapter 2 Getting Started with the Kernel
* Chapter 3 Process Management
** The Process
   + 进程简言之就是正在执行的程序和它所占有的资源的总和, 它所占有的资源,
     简单来说有如下:
     1) 打开的文件
     2) 挂起的信号
     3) 内核内部的数据
     4) 处理器的状态
     5) 内存地址(包括内存的映射关系)
     6) 一个或多个线程
     7) 存放全局变量的数据段
   + 线程一般是进程的一部分,但是在Linux系统中不区分进程和线程,对于Linux来说,
     线程只是碰巧恭喜资源的一些进程
   + 现代操作系统提供了两个虚拟:
     - 让所有的process认为自己独享CPU
     - 让所欲的process认为自己独享内存
   + 所有的process都是从fork()开始的, fork()这个函数很特别,从内核中返回两次,一次
     在parent,一次在child.当前的Linux是通过clone()来实现fork()的
   + 在chlid中,fork返回后,一般都是执行新的程序, exec()系列函数负责这个部分
   + 当process结束的适合,会调用exit()函数. 所有调用了exit()的process都要进入
     一个叫做zombie的状态来等待他们的parent调用wait()系列函数来释放他们的资源
** Process Descriptor and the Task Structure
   + 每一个process的信息都写在process descriptor里面(descriptor是一个unix理念,
     基本所有的对象都有一个descriptor), 在Linux里面process descriptor的类型不再
     是一个简单的类型,而是一个struct: struct task_struct
   + 在32位机器上面,这个task_struct有1.7K大小,因为他保存了一个process所占有的所有
     的资源(1打开的文件,2挂起的信号...)
** Allocating the Process Descriptor
   + 为了达到对象重用和缓存着色(Cache Coloring)的目的, Linux使用slab allocator
     来动态的分配task_struct.
   + 每个进程都有一个自己的进程内核栈,原来task_struct是静态分配的,那么就存放在每个
     内核栈的最后面(最不可能被用到的地方, 同时也方便找到).因为2.6以后, task_struct
     是动态分配的,地址都是不定的.所以很自然的我们想到在原来内存栈最后面的地方存放一个
     指针来指向task_struct.后来为了增加信息把这个指针扩充成了一个新的结构体
     task_info(这个结构体第一个成员就是指向task_struct的指针)
   + task_info代码如下:
     #+begin_src c
       struct thread_info {
           struct task_struct      *task;
           struct exec_domain      *exec_domain;
           __u32                   flags;
           __u32                   status;
           __u32                   cpu;
           int                     preempt_count;
           mm_segment_             addr_limit;
           struct restart_block    restart_block;
           void                    *sysenter_return;
           int                     uaccess_err;
       }
     #+end_src
   + 整体示意图如下:
     #+begin_example
                     Process Kernel Stack
              +------------------------------+ -- highest memory address
              |       Start of Stack         |
              |              |               |
              |              |               |
              |              |               |
              |              |               |
              |              |               |
              |              |               |
              |              |               |
              |              |               |
              +--------------+---------------+ -- stack pointer
              |              |               |
              |              V               |
              |                              |
              |                              |
              |                              |
              |                              |
              +------------------------------+
              |   struct thread_info {       |
              |     struct task_struct *task;|
              |     //.......                |
              |   }                          |
              |                              |
              +------------------------------+ -- lowest memory address
     #+end_example
** Storing the Process Descriptor
   + 每一个进程都是有一个ID来标示的,类型是pid_t为了和老的Unix系统兼容, pid_t通
     常都是int,这也就意味着系统同时最大共存的进程数目是32768
   + 如果希望能有更多的进程共存,那么可以修改/proc/sys/kernel/pid_max
   + 内核中河task_struct直接沟通的情况非常多,那么直接能够得到当前的task_struct
     就变的很必要.
     - 在power pc等寄存器多的系统中,一般当前的task_struct地址就存在r2这个寄存器中.
     - 在x86这种系统中,我们可以用刚才上面讲解的thread_info在进程内核栈最开始这个有
       利的布局,通过当前内核栈开始位置减去内核栈大小就得到了thread_info的位置,然后
       current_thread_info()->task就得到了task_struct的地址.
** Process State
   + task_struct里面有很多重要的变量,比如这个state,它表达了process可能处于的如下
     几种状态:
     - TASK_RUNNING : 正在运行或者在run-queue上等待运行
     - TASK_INTERRUPTIBLE : 程序正在睡眠(也就是被阻塞了),等待某个condition出现
       继而可以重新进入TASK_RUNNING状态.因为是INTERRUPTIBLE,所以proceess也可能
       接受到某个信号而重新进入TASK_RUNNING
     - TASK_UNINTERRUPTIBLE : 和上面的TASK_INTERRUPTIBLE一样,除了无法接受中断
       而唤醒.(我们使用ps命令看到的在state里面显示为D的,就是这种状态的process, 因
       为连信号都无法中断它,所以你发送SIGKILL也是无法杀掉这个进程的.即便有办法杀掉,
       杀掉这样的进程也不合理,因为它可能正在一个很重要的操作中间,而且可能持有一个信
       号量. 使用场景:
       1) 在一个process必须等待而不能被中断的场景
       2) 要等待的context应该会马上到来
     - __TASK_TRACED : 进程正在被其他进程跟踪
     - __TASK_STOPPED : 进程的执行已经被终止:
       1) 要么是收到SIGSTOP, SIGTSTP, SIGTTIN, SIGTTOU
       2) 要么是在debug的适合收到任何的信号
   + 这些状态的转换图如下:
     #+begin_example
       +-----------------+                                    +-----------------+
       |Existing task    |                                    |Task is          |
       |calls fork() to  |                                    |Terminated       |
       |create new task  |   Scheduler dispatches task to run |                 |
       |                 |   schedule() calls context_switch()|                 |
       +---+-------------+ +-------------------------------+  +-------------^---+
           |fork()         |                               |                |
           |    +----------+------+                +-------V---------+      |
           |    | TASK_RUNNING    |                |  TASK_RUNNING   |      |
           |    | (ready but not  |                |   (running)     |      |
           +---->  running)       |                |                 +------+
                |                 |                |                 |do_exit()
                +-^-----^---------+                +-------+--------++
                  |     |Task is preempted by higher prior |        |
                  |     +----------------------------------+        |
                  |                                                 |Task sleeps
                  |                                                 |on wait que
                  |        +-----------------------------+          |for a given
                  |        |  TASK_INTERRUPTIBLE         |          |event
                  |        |        or                   |          |
                  +--------+  TASK_UNINTERRUPTIBLE       |          |
        Events occurs, tsk |                             <----------+
        is woken up, placed+-----------------------------+
        back to runqueue.
     #+end_example
** Manipulating the Current Process State
   + 在Linux中要考虑多核的情况,所以不能直接task->state = state, 需要一个函数来完成
     #+begin_src c
       set_task_state(task, state);
       set_current_state(state);
     #+end_src
** Process Context
   + 所谓"进程上下文"是指的,当process开始运行的时候一般是在用户空间, 而用户空间的功能
     是有限的,它会用到内核的功能,这个时候可以通过:
     1)系统调用
     2)触发异常
   + 当我们通过上面的方式进入内核态的时候,内核其实是帮用户态的process在工作,我们就说
     这种工作状态为"进程上下文"(内核代表进程执行)
   + 在进程上下文中, current宏是有效的, 因为内核是知道在帮助哪个用户的process在工作,
     后面讲到的中断上下文(interrupt context), current就没有用了,因为不是为某个
     proces工作
** The Process Family Tree
   + 每个进程都有一个父进程, 而每个进程也会有0到多个子进程.在task_struct里面分别通过
     变量parent 和 child来访问
   + 所有的进程都有一个"跟进程"
     那就是init, 在Linux里面叫init_task,PID==1.
** Process Creation
   + 在类Unix系统中, 进程的创建都很独特: 其他操作系统一般是在新的用户空间创建新
     的进程. 而Unix把这个工作分成了两步:
     1) Fork: 创建一个子进程,只有一些参数和父进程不一样(parentID, PID不会继承挂起的信号也不会继承)
     2) exec():把新的可执行程序导入到地址空间开始执行
** Copy-on-Write:
   + 我们说Unix系统进程创建之所以独特,也就是独特在fork上面,因为传统的fork会吧所有
     的父进程的资源复制一遍给子进程.因为子进程并不需要那么多的资源,而且很可能
     子进程要重启新的炉灶(新的可执行程序),所有的资源都要放弃. 这个时候我们就
     引入了"写时拷贝"
   + 所谓"写时拷贝"就是fork的时候,资源是只读的,父子进程共享地址, 一旦这些资源被
     写入了,那么说明一份新的数据会诞生.那么父子就不能在使用相同的地址了. 这个时候
     就要再复制一份新的数据. 因为很多情况下, fork之后马上是exec(), 所以数据的复
     制从头到尾都没有执行过.
   + fork其实只需要给子进程复制一下page table, 和创建一些新的process descriptor
     里面的变量而已,所以速度要快,这符合Unix快速创建进程的哲学
** Forking
   + Linux的fork==>clone(), clone()通过一些flag来判断父子进程共享哪些资源
   + clone()内部==>do_fork()
   + do_fork()==>copy_process(), 真正的工作在这里执行:
     - 调用dup_task_struct(), 创建如下:
     - 检查一下创建新进程后,当前用户的资源有没有超标
     - 因为刚才的数据都是复制的,父子的task_struct是一样的,但是为了区分父子,肯定有很多属
       性要不一样:比如一些计数的属性要清零. 但大部分还是一样的.这里主要是属性,不是
       "写时拷贝"的那些数据
     - 子进程被设置为TASK_UNINTERRUPTIBLE防止它运行
     - copy_process()==>copy_flag() 更新子进程的flags: 清零PF_SUPERPRIV(表明进程是
       否拥有超级用户权限), 清零PF_FORKNOEXEC(表明还没有调用exec())
     - 根据传入clone()的flag的不同, copy_process()会选择对以下的资源才去复制一份给子
       进程,还是共享(一般来说如果是thread的话,那么这些信息是共享的,在Linux系统中就是如
       果几个process共享这些信息,那么他们就是thread):
     - copy_process()会释放资源然后返回给调用者一个指向新子进程的指针.
   + copy_process()<==do_fork()完成任务返回了do_fork, 如果copy_process()调用成功
     那么新的子进程会比父进程先唤醒, 这是基于这样一个事实: 如果父进程先唤醒,那么可能会
     写入address space, 这样一来,就会调用写时拷贝来复制一份资源!如果子进程先调用,那么
     这些内容本是可以避免的.因为子进程可能上来就放弃了这些资源,调用exec()开始新的生活!
** vfork()
   + vfork()是一种在copy-on-write技术诞生之前的应对"两步生成新process"的方案, 现在
     已经不需要了.Linux在2.2之前vfork()甚至只是通过fork()实现的.
   + vfork()主要是添加了很多限制:
     1) 子进程和父进程公用地址空间(页表不创建新的)
     2) 知道子进程退出或者执行exec(), 父进程都是被阻塞的.
     3) 子进程不允许写入到地址空间里面.
** The Linux Implementation of Threads
   + 在Linux中, 线程只是碰巧共享一些资源(比如地址空间)的一群进程
** Creating Threads
   + 线程只是在创建的时候传给clone()不同的flage而已,比如我们要创建一个子线程(新
     创建的子进程和原来的父进程共享资源,他们一块叫做线程):
     #+begin_src c
       /**************************************************************/
       /* CLONE_VM -> Share address space                            */
       /* CLONE_FS -> Share file system information                  */
       /* CLONE_FILES -> Share open files                            */
       /* CLONE_SIGHAND -> Share blocked signals and signal handlers */
       /**************************************************************/
       clone(CLONE_VM | CLONE_FS |CLONE_FILES |CLONE_SIGHAND, 0);
     #+end_src
** Kernel Threads
   + 内核线程也是进程的一种,也有task_struct,也会被调度,也有那么多state, 它特别的
     地方在于
     - 它没有address space(task_struct的成员mm指针为NULL)
     - 它不会被交换到用户态
   + 内核线程只能由其他内核线程fork创建,在bash里面可以通过ps -ef来查看他们.
** Process Termination
   + process也要消亡,其方式也就是主动和被动两种:
     - 主动调用exit(), 或者被其他语言比如C,在main的最后加上一个exit()
     - 收到了一个信号,或者异常,自己无法处理,又不能忽略
   + 所有的进程结束都是通过do_exit()来处理的
     - 首先是标记flags为PF_EXISTING,表示正在退出
     - 释放掉资源,内存,文件,信号量等等
     - 设置exit code
     - 通知父进程,state成为EXIT_ZOMBIE
     - 调用schedule()执行其他程序, 因为当前程序不可调度了,所以这个函数永远都
       不会返回.
   + 到这个阶段,已经进入了EXIT_ZOMBIE状态,也不能run了, resource也都没有了.这个时候
     一个进程所占有的资源就剩下传统三强了:
     1) 内核栈
     2) thread_info(其实也是在内核栈里面)
     3) task_struct 结构体
** Removing the Process Descriptor
   + 在do_exit()完成后,很多信息都已经删除了,但是还是保留了task_struct,就是因为希望在
     进程退出后,依然能够得到关于它的消息, 当父进程得知子进程退出后,会调用wait()
     来释放最后的这些资源
   + wait()函数的标准动作是挂起,以等待其中一个子进程返回, 同时会返回结束进程的PID, 从
     参数中返回的指针还能知道子进程的exit code
   + 当真要释放process descriptor的时候, release_task()会被调用:
     1) release_task()==>__exit_signal()==>__unhash_process()==>detach_pid(),
	把进程从pidhash已经task list中删除
     2) __exit_signal()会释放已经退出的进程的所有资源
     3) 如果这个是thread group的最后一员,而且leader已经zombie了,那么就通知zombie
        leader的父进程
     4) release_task()==>put_task_struct()会把传统三强铁山角(内核栈, thread_info,
        以及task_struct)释放掉
** The Dilemma of the Parentless Task
   + 如果父进程在子进程之前就退出了,那么我们可以:
     1) 从当前的thread groupd里面找一个进程做自己的父进程
     2) 让PID=1的init来做自己的父进程
* Chapter 4 Process Scheduling
** Multistasking
   + 多任务操作系统能在单核计算机上展现出所有进程共同运行的假象, 在多核计算机上,
     让多个进程真正并行的进行运算
   + 多任务操作系统分成两类:
     - 飞抢占式多任务(cooperative multitasking):调度器能决定哪个进程结束,哪个
       进程开始.
     - 抢占式多任务(preemptive multitasking):进程一旦开始运行,就只有它自己主动
       让出cpu, 其他进程无法抢占
   + 绝大部分操作系统都是抢占式的,Linux也是
** Linux's Process Scheduler
   + 早期linux的调度设计方法简单
   + 2.5开始设计出O(1)调度算法,能够很好的应多多核
   + 后来发现O(1)调度算法对于人机交互的进程很不友好,最终引入了CFS(Completely
     Fair Scheduler)
** I/O-Bound Versus Processor-Bound Processes
   + 所谓I/O-Bound的进程就是真正运行的不多,总是在等IO的进程.这种进程调度的时候
     就要多给他机会,但是每次时间都不要太长
   + 所谓Perocessor-Bound的进程,就是每次调度都是在不停的运行, 这种进程调度的时
     候要每次多给时间, 但不要多给机会.
** Process Priority
   + 最常见的调度算法就是"基于优先级的调度", 就是把所有的进程都根据其价值需求,进行
     分机.优先级高的运行的早,运行时间多
   + 这种最朴素的理念在Linux中也有体现, Linux有两中优先级值:
     - nice value : 默认值是0, 区间是[-20, 19], 值越大就是对其他人越nice, 也就
       优先级越低. 不同Unix系统对于nice的利用不同. Mac OSX 根据nice值来确定运行
       时间. Linux根据nice值来确定处理器使用的比例.
     - 实时优先级 :区间是[0-99], 这个是值越大优先级越高. 跟nice是两个不同的系统.
       任何实时进程优先级都大于普通进程
** Timeslice
   + 我们前面说过传统的操作系统会给每个进程一个timesilce, 而timeslice越长,交互
     程序的体验就越差,所以现在操作系统中的时间片都非常的小--比如10微秒
   + Linux不是分配时间片的,而是分配cpu使用比例,分配的原则:nice 值为主,大家都要兼顾
     也就是说一个进程能获得多少cpu使用比例,要
     1) 看自己的nice值是不是够高
     2) 还要看系统中其他的进程跟自己比起来值大还是小
     3) 系统中进程多不多.僧多粥就少
   + 大部分操作系统下,当某个进程进入可执行状态的时候,看它是不是取代当前的进程运行.主要
     是看它的优先级,以及时间片是否足够
   + Linux没有时间片的概念.进程是否运行是看它使用的CPU的比例,如果新加入可运行的进程
     CPU使用比例比当前的进程低,那么马上就能运行.
** The Scheduling Policy in Action
   + 举个例子.一个文字编辑器和一个视频解码器,前者是IO bound, 后者是CPU bound, 假设
     系统中只有他们俩,而且nice值相同,那么原始情况下没人的cpu 使用比例都是50%. 开始假设
     编辑器先运行, 很快它就会等待IO让出CPU, 所以cpu使用比例也没多少==>1 % now. 视频
     解码马上开始运行. 然后占用的cpu比例很大==> 35%. 这个时候文字编辑器又收到了IO返回
     因为他的cpu使用率远远小于视频解码器,所以它能立刻抢占视频解码器.
** Scheduler Classes
   + Linux的调度器是模块化的.好让不同的调度器去调度不同类型的进程
** Process Scheduling in Unix Systems
   + 传统的Unix调度比较粗犷.每个process给个时间片和优先级,这样做有很多问题:
     1) 因为有级别和时间片,优先级高的进程和优先级低的进程共存的时候没问题.但是如果所有进程
        都是低优先级,那么调度就会非常频繁(比如刚才编辑器解码器那个例子,如果两者时间片很短,
        那就调度台频繁了.Linux肯定没这个问题嘛,一人50%)
     2) 数据分布不均匀. nice值在尾端的进程之间时间片差距太大.
     3) 是节拍器的整数倍,导致进程间的差距不一
     4) 给交互进程开了后门.
** The Linux Scheduling Implementation
   + 下面介绍CFS的四个部分
*** Time Accounting:
    + 虽然CFS不再有时间片的概念,但是还是要记录用掉的时间,一遍能保证大家相对的公平, CFS
      使用shed_entity来记录进程数目:
      #+begin_src c
        struct sched_entity {
            struct load_weight      load;
            struct rb_node          run_node;
            struct list_head        group_node;
            unsigned int            on_rq;
            u64                     exec_start;
            u64                     sum_exec_runtime;
            u64                     vruntim;
            u64                     prev_sum_exec_runtime;
            //....
        }
      #+end_src
    + sched_entity存在在task_struct里面叫做se.
*** The Virtual Runtime
    + 上面se其中一个成员变量是保存进程的虚拟运行时间的.他是实习运行时间加权运行进程数目
      得到的值.
    + virtual runtime的单位是ns, 和timer的节拍就没有关系了. 这个vruntim的设计是为了
      大约估计每个process的运行虚拟时间.因为我们的多核cpu不是理论中那么完美,理论情况下
      同一个权限的process应该一直都有一样的vruntim, 也就不用再去计算,然后相互平衡了.
    + 每次每个process变成runable, unrunnable或者block的时候, 就会调用update_curr()
      来进行修正se里面的vruntime.
*** Process Selection
    + 我们知道, 因为我们不可能实现完全的多任务,所以Linux为了平衡process, 采取了非常
      简单的选择process的策略:谁的virtual time最小下面就先让谁运行.
    + CFS 使用了红黑树来管理所有的runnable的process, 可以迅速的找到有最小vruntime值
      的process.
*** Picking the Next Task
    + Linux使用函数__pick_next_entity()来找到下一个运行的process, 从代码中我们可以看
      到每次并不是真的找到红黑树最左下的进程,这个值其实是被rb_leftmost变量缓存的
      #+begin_src c
        static struct sched_entity *__pick_next_entity(struct cfs_rq *cfs_rq) {
            struct rb_node *left = cfs_rq->rb_leftmost;
            if (!left) {
                return NULL;
            }
            return rb_entry(left, struct sched_entity, run_mode);
        }
      #+end_src
    + 如果leftmost什么值都没有的话,返回NULL, 这个时候CFS会去调度idle task
*** Adding Process to the Tree
    + 没当有新的process被创建,或者process被唤醒的时候, 我们要把新的proces加入到红黑树
      里面去, 这个红黑树的key就是vruntime,
    + 这个把process加入红黑树的函数和其他没有什么区别, 唯一不同的是,我们不在乎vruntime
      冲突,一样的vruntime那就放到一块.
*** Removing Processes from the Tree
    + 当有process介绍, 或者process被block的时候, 我们i就可以从红黑树里面移除这个
      process
*** The Scheduler Entry Point
    + 调度的核心函数是schedule(), 定义在kernel/sched.c, 而schedule()函数主要的
      工作都是由pick_next_task这个函数完成的
      #+begin_src c
        /*
         ,* pick up the hightest-proi task
         ,*/
        static inline struct task_struct *
        pick_next_task(struct rq *rq)
        {
            const struct sched_class *class;
            struct task_struct *p;
            /*
             ,* 下面这段代码是说,我们总共有nr_running个process在跑,如果cfs也有
             ,* 这么多的process在跑,说明所有的进程都是CFS在调度,都是普通进程,没有
             ,* real time的(这也是大多数的情况)
             ,*/
            if (likely(rq->nr_running == rq->cfs.nr_running)) {
                p = fair_sched_class.pick_next_task(rq);
                if (likely(p))
                    return p;
            }

            class = sched_class_highest;
            for (; ;) {
                /*
                 * 每个class里面的pick_next_task和总的这个不是一个函数,比如CFS
                 * 就是用pick_next_entity()来实现pick_next_task()的.
                 */
                p = class->pick_next_task(rq);
                if (p) {
                    return p;
                }
                /*
                 ,* 不可能返回NULL值, 因为总有一个叫idle 的 schedule class
                 ,*/
                class = class->next;
            }
        };
      #+end_src
*** Sleeping and Waking Up
    + 一旦process不想运行,就sleep, 然后就会把自己从调动红黑树中删除,加入自己到等待队列
      (Wait Queue), 然后调用schedule()
    + 一旦被唤醒那就是相反, 设置runnable, 从等待队列删除, 插入红黑树,是否调用schedule那
      就暂时不知道了
*** Wait Queues
    + 在内核中,进入睡眠是通过把自己加入到等待队列里面, 睡眠和唤醒的实现要非常小心, 因为很
      可能会引入竞争.
      #+begin_src c
        /* 'q' is the wait queue we wish to sleep on */
        DEFINE_WAIT(wait); // create one queue node statically

        add_wait_queue(q, &wait);
        while (!condition) { //防止虚假唤醒存在
            prepare_to_wait(&q, &wait, TASK_INTERRUPTIBLE);
            if (signal_pending(current)){
                /* handle singnal */
            }
            //肯定要交出cpu了,因为自己的state已经不是runable了,
            //vruntime再小也不会轮到自己运行了.
            schedule();
        }
        //condition发生了,重新调度到自己,把自己解放出来
        finish_wait(&q, &wait);
      #+end_src
*** Waking Up
    + 我们是通过wake_up()来激活某一些特定的进程的(它们通常加入到同一个queue里面)
    + wake_up()==>
      1) try_to_wake_up(), 设置TASK_RUNNING
      2) enqueue_task(),加入到红黑树
      3) 设置need_resched, 如果有更需要运行的进程.
    + 一般谁引发某个event,它就会来调用wake_up()比如数据从硬盘上来的实惠, VFS会
      调用wake_up()
    + 一个process被wake up,并不代表着它所等待的event就发生了,因为存在着虚假唤醒
      (Spurious wake-ups): 也就是说存在一种可能,我们的process被唤醒了,但是其实它
      所等待的condition还没真的发生, 所以我们要用一个while循环来保护, 在wait退出
      以后继续检查一遍condition,double check一下.
    + 这里的while保护机制,其实是多线程编程的要求. 而不仅仅是内核这么做.比如在下例
      中, 正常情况下, 其他thread肯定会先设置full为true, 然后出发signal, 但是:
      1) 在多线程的情况下,并不总是线性运行
      2) 程序员可能犯错误,把设置为full写到signal后面,我们多做一次check何乐而不为呢.
      #+begin_src c
        /* In any waiting thread */
        while (!buf->full) {
            wait(&buf->cond, &buf->lock);
        }

        /* In any other thread; */
        if (buf->n >= buf->size) {
            buf->full = 1;
            signal(&buf->cond);
        }
      #+end_src
    + 下面就是运行和睡眠状态的转换情况情况总结
      #+begin_example
           __add_wait_queue() adds task to a wait queue, sets the task's
           state to TASK_INTERRUPTIBLE, and calls schedule(). schedule()
           calls deactivate_task() which removes the task from the runqueue
           +---------------------------------------------------+
		   |												   |
		   |												   |
		   |												   |
		   |												  \|/
		+--+-----------+								  +----.------------+
		|			   | and task executes signal handler |					|
		| TASK_RUNNING |<--------------------------------->TASK_INTERRUPTIBLE
		|			   |								  |					|
		+--.-----------+								  +----+------------+
		  /|\												   |
		   +---------------------------------------------------+
            Event the task is waiting for occurs, and try_to_wake_up()
            sets the task to TASK_RUNNING, calls activate_task() to
            add the task to a runqueue, and calls schedule().
            __remove_wait_queue() removes the task from the wait queue.
      #+end_example
*** Preemption and Context Switching
    + schedule()==>context_switch()进行上下文切换:
      - 调用switch_mm(), 切换新老task的virtual memory mapping
      - 调用switch_to(), 切换新老task的处理器state
    + 一个task不能完全靠自觉性来决定自己占用cpu多久.每当need_resched flag被设置
      就必须进行进程切换了,这个flag不可能被正在运行task进行设置, 设置的肯定是能和
      cpu同时运行的其他部件:
      - 频率始终可以通过scheduler_tick()来设置这个flag, 这种情况通常是process
        运行的够长了.
      - 外部的中断处理可以通过try_to_wake_up()来设置个flag, 这种情况是有新的
        优先级更高的task出现了
    + 返回用户空间,或者从终端返回的话,都要去检查need_resched.
    + need_resched这个是每个process的task_struct里面都有一份的.虽然全局又一个
      flag就可以了,但是每个process都有一个的话,用起来更快(current的存在)
*** User Preemption
    + 当从内核空间跳入到用户空间的时候(要么从system call返回,要么是从interrupt返
      回), 会去检测need_resched, 因为这个时候kernel转到进程A是安全的,那么如果需
      要调度,我转到进程B也肯定是安全的.
*** Kernel Preemption
    + 和其他Unix变体不同, Linux的内核也是可抢占的.任何"安全状态"下的kernel进程
      (也就是kernel在为某个进程服务而运行,比如为某个进程运行system call)都是可
      以被抢占的
    + 所谓"安全状态"下的kernel是指:没有lock
    + 从2.6开始支持抢占内核,为此在每个thread_info里面增加了一个变量preempt_count
      来记录当前task拥有的lock.每当此数字为0的时候,内核就是可抢占的.
    + 如果从interrupt返回到用户空间的话,那么check need_resched然后决定是否调度
      就是前面讲的user preemption, 如果interrupt返回到内核空间的话,如果
      preempt_count为0的话,说明没lock,此时也是可以check need_resched调度的,
      如果不需要调度,那么就返回原本该返回的位置.
    + 也可以显式调用内核抢占,比如当在内核中的task被block,或者调用schedule()的时候,
      这种情况下, 抢占是一直发生的.无论锁的情况怎样.
    + 总结一下内核抢占发生的情形就是:
      - 当有中断处理退出并且中断处理返回的是内核空间
      - 当内核变的可以被抢占preempt_count为0
      - 当内核task显式调用schedule(), 自愿放弃cpu
      - 当内核task被block(也就会导致schedule())

      

*** Real-Time Scheduling Policies
    + 常规的调度当然是使用调度策略SCHED_NORMAL, 使用的调度器是Completely Fair
      Scheduler.
    + 实时进程有两种调度策略SCHED_FIFO(先到先服务), SCHED_RR(轮循), 使用一个在
      kernel_sched_rt.c中定义的实时调度器. 实时进程永远比SCHED_NORMAL的优先级
      要高
    + SCHED_FIFO 就是一旦有个进程运行,除非有更高优先级的进程,否则当前进程将一直
      持有CPU,直到它自愿放弃. 高优先级的进程运行完才轮到低优先级运行.
    + SCHED_RR 和SCHED_FIFO类似,只不过是每个进程有一个时间片,如果一个进程用完了
      时间片,那么它必须让给相同优先级的其他进程.
    + Linux提供的是软实时性, 所谓软实时性,就是不保证在一定的时间内(时间很短)完成
      高优先级进程抢占低优先级进程的调度
    + 实时调度的优先级是0-99, SCHED_NORMAL的优先级是100-139(对应nice值-20到19)
*** Processor Affinity System Calls
    + Linux可以保证某些进程只在cpu的某几个核中运行, 在task_struct 中有个叫做
      cpus_allowed的成员变量,其中的每个bit都是代表一个cpu核心, 默认状态下每个
      bit都是1, 也就是process可以运行在各个核中.
*** Yielding Processor Time
    + Linux 提供了sched_yield(), 当前运行的进程可以通过它来放弃运行(并且很可能
      在一段时间内都不能运行), 其原理是把当前进程从active array移动到expired 
      array.
    + 因为实时进程从来不expire,所以实时进程调用sched_yield()的时候是把进程停止
      然后插入到priority list的最后.
* Chapter 5 System Calls
** Communicating with the kernel
   + 系统调用提供了一个连接用户空间进程和硬件的过渡层,这个层出现有下面几个原因:
     - 提供抽象的硬件接口, 用户不需要关心硬件的类型
     - 系统调用保证了系统的安全性和稳定性
     - 能为进程提供虚拟化.
** APIs, POSIX, and C LIbrary
   + API就是程序使用的接口, 这些接口可以用零个,一个或者多个系统调用实现
   + POSIX这种API的一个事实上的标准
   + 系统调用的接口,在Linux上面有一部分是通过C library来实现的.
   + 从编程者的角度来看, system call是没有什么关联的.程序员关心的是API.
   + 内核只是关系system call, 至于其他API如何使用system call,它不关心.
** Syscalls
   + 在Linux中, system call被叫做syscalls, 调用者看起来就像是C语言的函数, 返回值
     为0通常表示成功,失败的话,会返回非零,并且在errno变量里面写入出错值, 这个值通过
     函数比如perror()可以被解读.
   + 我们来看getpid()的实现
     #+begin_src c
       SYSCALL_DEFINE0(getpid)
       {
           // returns current->tgid,  thread group id in Linux is PID
           return task_tgid_vnr(current); 
       }
     #+end_src
   + SYSCALL_DEFINE0是用来定义没有参数的系统调用的宏,展开来就是
     #+begin_src c
       asmlinkage long sys_getpid(void)
     #+end_src
   + asmlinkage 是告诉编译器只看本函数参数的stack,是所有系统调用的标配.
   + long是为了32位64位兼容
   + getpid()在内核中被定义为sys_getpid(). 这是命名常规,内核都会加上个sys_
** System Call Numbers
   + 每个系统调用都有自己的一个编号的,在被使用的时候都是用编号的,所有编号不能变
   + 为了应付不常出现的编号缺失等问题, Linux发明了sys_ni_syscall(),
     它什么也不干,就是返回-ENOSYS
** System Call
   + Linux上面的系统调用非常迅速,这得益于Linux上面快速的上下文切换,以及西东调用
     处理的简洁
** System Call Handler
   + 如果用户空间的进程想做些高难度自己没权限的操作,比如读取文件,那么它就要告诉
     内核,所用的方法就是:软件中断(也就是exception, 因为软件中断是cpu可以预见的
     前面一个指令运行完才可能发生的中断, 而硬件中断,也就是interrupt,是异步的
     因为不是cpu发出的,是其他外在硬件发出的,可以在任何时间点出现)
   + 系统中断在x86系统中就是128号中断, 代码是int $0x80. 这段汇编过后, 程序就
     算进入kernel了, 会跳到128号中断的handler处(又叫system_call(), 原来这
     个代码通常在x86系统上是在entry_64.S上实现的)
** Denoting the Correct System Call
   + 光光进入kernel模式是没用的, 所以进入内核的时候,要带着系统调用号. x86系统
     中, 这个系统调用号是存在eax 寄存器里面的.
   + 在kernel mode要检查这个数是不是待遇NR_syscalls, 如果大于就出错,否则就去
     system call table里面找到相应的函数指针, 然后去调用.
** Parameter Passing
   + 函数调用除了函数指针,还得有参数, 在x86系统中, 通常前五个参数分别存在ebx, 
     ecx, edx, esi和edi, 如果六个或者六个以上的参数,通过一个寄存器传递一个
     指向用户空间内存的指针.
   + 返回值还是通过寄存器,比如eax, 返回给用户空间
** Implementing System Calls
   + system calls要考虑的事情很多,因为一旦施行,很多system call几十年都没有变
   + system call一定要检查参数, 而且一些操作要运用特别提供的函数(函数会保障运行
     的错误检查)比如从用户空间拷贝,使用copy_from_user, 拷贝到用户空间,使用
     copy_to_user
** System Call Context
   + 前面说过了, system call是kernel替某个process在执行, 那么kernel在执行的时候,
     current肯定指向调用system call的process
   + 就算是在kernel执行,由于kernel也是可被抢占的,而新执行的task很可能也会用到刚刚
     被抢占的system call,这就需要kernel要设计成"可重入(reentrant)"
   + 每写完一个system call的时候,还需要如下三步来使得system call起作用:
     - 在sys_call_table里面加一个新的item
     - 在<asm/unistd.h>里面加一个syscall number
     - 把syscall编译进内核.
   + system call写完之后, 会用C库在外面包一层,使用者只使用C库就可以调用这些syscalls
     了,也就是说,用户只看到C库,看不到syscall
   + C是通过Linux提供的_syscalln()这个宏来调用syscall的,比如下面是如何在c中调用
     open syscalls
     #+begin_src c
       /* syscall defined by kernel */
       long open(const char *filename, int flags, int mode);
       
       /* Macro to use this syscalls */    
       _syscall3(long, open, const char *, filename, int, flags, int, mode);   
     #+end_src
** Why Not to implement a System Call
   + 说了这么多,还是不建议你自己实现系统调用.
* Chapter 6 Kernel Data Structures
* Chapter 7 Interrupts and Interrupt Handlers
  + interrupt叫做中断,是硬件发出的(exception叫做异常, 是软件发出的). 之所以设计
    interrupt是因为相比于cpu的运行速度,硬件的运行速度慢的不可理喻,cpu不可能发送一
    个要求给硬件,然后等它. 它要去忙别的事情, interrupt就是用来让硬件通知cpu, "我
    好了,下一步怎么办?"
** Interrupts
   + interrupts是由硬件发出的,而且每个中断都有一个interrupt value.使得操作系统
     能够知道来的是哪个硬件发出的中断
** Interrupt Handlers
   + 操作系统对相应的中断做出反应的函数叫做interrupt handler,又叫ISR(interrupt
     service routine)
   + interrupt handler是属于内核处理设备的部分,叫做device driver
   + 前面说了interrupt handler其实就是function, c语言写的.那kernel的其他部分
     也是c function, 他们有什么不同么? 不同就是这些interrupt handler存在于一个
     特殊的context, 叫做interrupt context, 这个context是原子的,也就是一口气
     执行完,不能被block的
   + 因为interrupt是随时来的,那么interrupt handler也就要随时调用,肆意抢占其他
     进程,又因为它不能被block, 那么handler的涉及势必要快,不能让其他进程等的太久
   + 而另一方面,很多时候kernel要"处理"的设备的请求不是那么快的,比如网络传输传很多,
     不是一会就能传完的
** Top Halves Versus Bottom Halves
   + 前面说到了handler涉及的一些悖论. 既要快,事情还多? Linux的解决办法是,先做"紧
     急且必要"的(top halves), 剩下的以后再做(bottom halves)
   + 以网卡发送的interrupt为例:
     - 网卡发送interrupt, kernel收到interrupt了以后,停下手中的工作,让相应的
       interrupt handler去处理.
     - interrupt handler 首先把网卡上的packet拷贝到内存, 然后通知网卡可以继续
       工作了. 这些工作是"紧急且必要的"因为如果不赶紧拷, 网卡可能会溢出.
     - 一旦内容被拷贝到内存, handler的top halves就结束了, 系统赶紧回到刚才运行
       的地方.
     - 剩下讲内存里面的数据怎么处理就是bottom half的事情了
** Registering an Interrupt Handler
   + 设备是通过驱动和操作系统发生联系的. 所以一个driver可以注册一个interrupt
     handler,注册的同时也就选定了一个interrupt line
     #+begin_src c
       /* rquest_irq: allocate a given interrupt line */
       int request_irq(unsigned int irq,
                       irq_handler_t handler,
                       unsigned long flags,
                       const char *name,
                       void *dev)
     #+end_src
   + 参数irq是指的interrupt number,对于传统设备来说, 这个数字往往是硬编码进去的.
     对于其他大多数设备,这个值是可以探测获取或者编程动态确定
   + 参数handler是一个指针,指向处理这个终端的函数,其原型如下:
     #+begin_src c
       typedef irqreturn_t (*irq_handler_t)(int, void *);
     #+end_src
   + 参数flag可以是如下flag的组合:
     - IRQF_DISABLED :如果为1, kernel会在处理它的时候,屏蔽掉所有中断(包括和他类型
       一致的其他中断), 如果为0, kernel会在处理它的时候,同时检查其他中断(但是和他
       类型一致的其他中断则还是屏蔽的, 不仅仅是同一类型,公用一个interrupt line的所有
       中断都会被屏蔽,这是为了公用interrupt line, 同时简化linux的设计.), 大多数
       handler不会设置这个flag,其存在是为了兼容原来的SA_INTERRUPT flag
     - IRQF_SAMPLE_RANDOM:如果为1, 那么这个中断的频率会为kernel的随机数池做贡献,
       如果是系统时钟等可产生固定频率的设备,请不要设置
     - IRQF_TIMER:这个flag说明handler处理系统时钟相关中断
     - IRQF_SHARED:这个flag说明当前handler使用的interrupt line是被多个handler所
       共有的.(因为一个interrupt line其实最好只有一个硬件, 既然Linux选择了share,
       那么丢失interrupt的事情肯定是会发生的)
   + 参数name是为了给用户看的.一个有意义的名字,比如键盘的handler就叫做keyboard
   + 参数dev是和前面的flag IRQF_SHARED配合使用的.当某个handler被释放的时候,如果
     它又是share interrupt line的,那么就需要dev来唯一确定它是哪一个(有点GUID的
     感觉), 这个参数是个void类型的指针, 实践中为了保持唯一性,一般传递driver 设备
     的struct 地址.
** Freeing an interrupt handler
   + 如果驱动卸载的时候,我们也要注销相应的中断处理程序. 如果是不shared的话,那么
     释放这个中断处理程序, interrupt line也就disable了. 如果是shared的话,那么要
     等到这条线上所有的中断处理程序都释放,才disable响应的interrupt line
     #+begin_src c
       void free_irq(unsigned int irq, void *dev)
     #+end_src
** Writing an Interrupt Handler
   + interrupt handler的参数和request_irq里面是一致的. irq表示interrupt number,
     dev还是那个GUID, 特别是在share interrupt line的情况下.
     #+begin_src c
       static irqreturn_t intr_handler(int irq, void *dev)
     #+end_src
   + 因为handler不会被其他文件的代码调用,所以设置为static(对其他文件不可见)
   + 返回值可以有两个:
     - IRQ_NONE : 说明interrupt handler在你的interrupt line上检测到了一个interrupt, 
       但是发生源却不是你所说的设备(dev)
     - IRQ_HANDLED: 说明interrupt handler在你的interrupt line 检测到了一个interrupt,
       而且dev这个设备发出的.
     - 两个变量设计的初衷,还是因为interrupt handler 共享了interrupt line. 通过返回值
       来判断是谁发出的interrupt. 如果一个interrupt line里面所有的handler都返回IRQ_NONE,
       那肯定是出问题了.
     - 在硬件设计上面,也要考虑这个问题,一定有硬件的支持才可能知道到底这个interrupt line
       上面发出的interrupt是谁发的.
     - 每当在interrupt line上面有interrupt的时候, kernel会迅速的调用某个interrupt line
       上面所有的handler. handler如果意识到不是自己发出的interrupt要迅速的退出. handler
       一般是通过检查硬件的寄存器来判断是不是自己发出的interrupt.
** Reentrancy and Interrupt Handlers
   + interrupt handler在linux 上面的设计是不可重入的.如果一旦interrupt line里面的任意一
     个handler开始运行,那么整个line都会被屏蔽,也就防止了同一个line上的其他interrupt的响应.
     这也就说明同一个interrupt不可能被嵌套调用(调用一个handler的时候,被中断去执行同样的
     handler的情况不存在), 这样简化了系统的设计. 当然也就意味着会丢失interrupt.
** A Real-Life Interrupt Handler
      
     
     

      
      
